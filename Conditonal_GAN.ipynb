{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kazuhiko-Miyata/conditional_GAN_MNIST_generator/blob/main/Conditonal_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-vG_dD4bmLx"
      },
      "source": [
        "## **Conditional GANの概要**\n",
        "DCGANはノイズ$z$から０〜９の数字の画像に生成した。このとき数字画像の生成はランダムで、数字の指定はできない。この欠点を補ったGANがConditional GAN。CGANはクラスの指定が可能で、クラス０〜９を指定した画像生成が可能になる。\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1x3CE2E07cSKO_F6TD1S2mi6Ps9ONwKJs\" width=\"70%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97JyjsNhdZSZ"
      },
      "source": [
        "CGANはGANと同じく、生成器と識別器の２つのネットワークで構成される。GANとの相違点は、クラスを指定する条件ベクトル$y$の存在。データ分布$p_{x\\sim{data}}(x)$から取得される画像を$x$、条件を$y$、ランダムノイズを$z$とすると、損失関数は次の式になる。\n",
        "\n",
        "<br><br>$$\n",
        "\\min_G  \\max_D  L(D,G)=\\mathbb{E}_{x\\sim{pdata(x)}}[\\log D(x|y)]+\\mathbb{E}_{z\\sim{p_z}(z)}[\\log(1-D(G(z|y)))]\n",
        "$$\n",
        "\n",
        "<br><br>第１行の$D(x|y)$は条件$y$を満たす画像$x$への入力、第２行の$G(z|y)$は条件$y$を満たすランダムノイズ$z$の生成器への入力と解釈できる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NDPU2EthKdV"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1ymMGw4QSg4XFJNl9_--qE88Y8AL6kNlO\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIc-AwlRhmGQ"
      },
      "source": [
        "DCGANはノイズ$Z$を生成器に入力したが、CGANはノイズ$z$にクラスを指定するOne-hotベクトルを結合したノイズを生成器に入力する。同様に、識別器に入力する本物画像にもクラスを指定する情報が必要になる、ただし、画像は３次元配列なので、ノイズのように単純に１次元のOne-hotベクトルを結合できない。そこでOne-hotベクトルを画像サイズに拡大したクラス数ぶんの条件画像をチャンネル方向に結合する。このとき、「すべての要素が１」の条件画像が１枚あり、残りは「すべて要素が０」の条件画像になる。\n",
        "<br>DCGANの実装と同じく、数字画像（MNIST）をCGANで生成する場合、本物画像と生成画像にクラス数１０の条件画像をチャンネル方向に結合した配列が必要になる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xySZAPYJkUtS"
      },
      "source": [
        "## **Conditional GANの実装**\n",
        "DCGANはチャンネル数１００のランダムノイズをB個用意した。CGANの場合、チャンネル数１００にクラス数１０を追加し、（B、１１０、１、１）のTensorを作成する。そのため、生成器の入力チャンネル数は１１０を指定する。\n",
        "\n",
        "<br>**▽生成器Gの定義**\n",
        "```\n",
        "# 生成器G。ランダムベクトルから生成画像を作成する\n",
        "netG = Generator(nz=nz+10, nch_g=nch_g).to(device) # 10はn_class=10を指す。出し分けに必要なラベル数\n",
        "netG.apply(weights_init)  # weights_init関数で初期化\n",
        "print(netG)\n",
        "```\n",
        "\n",
        "<br><br>DCGANの識別器に入力するTensorは（B、１１、２８、２８）のTensorを入力する。そのため識別器の入力チャンネルは１１になる。\n",
        "\n",
        "<br>▽**識別器Dの定義**\n",
        "\n",
        "\n",
        "```\n",
        "# 識別器D。画像が本物か生成かを識別「する\n",
        "netD 0 Discriminator(nch_h=1+10, nch_d=nch_d).to(device) # 10はn_class=10を指す。分類に必要なラベル\n",
        "netD.apply(weights_init)\n",
        "print(netD)\n",
        "```\n",
        "\n",
        "<br><br>クラスラベルをOne-hotベクトルに変換する関数を定義する。\n",
        "\n",
        "<br>▽**One-hotベクトルの変換**\n",
        "\n",
        "\n",
        "```\n",
        "def onehot_encode(label, device, n_class=10):\n",
        "    \"\"\"\n",
        "    カテゴリカル変数のラベルをOne-hot形式に変換する\n",
        "    :param label: 変換対象のラベル\n",
        "    :param device: 学習に使用するデバイス。CPUあるいはGPU\n",
        "    :param n_class: ラベルのクラス数\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    eye = torch.eye(n_class, device=device)\n",
        "    # ランダムベクトルあるいは画像と連結するために（B, c_class, 1, 1）のTensorにして戻す\n",
        "    return eye[label].view(-1, n_class, 1, 1)\n",
        "```\n",
        "<br><br>識別器に入力する画像にラベルを追加する関数を定義\n",
        "\n",
        "<br>▽**画像とラベルの連結**\n",
        "\n",
        "\n",
        "```\n",
        "def concat_image_label(image, label, device, n_class=10):\n",
        "    \"\"\"\n",
        "    画像とラベルを連結する\n",
        "    :param image: 画像\n",
        "    :param label: ラベル\n",
        "    :param device: 学習に使用するデバイス。CPUもしくはGPU\n",
        "    :param n_class: ラベルのクラス数\n",
        "    :return: 画像とラベルをチャンネル方向に連結したTensor\n",
        "    \"\"\"\n",
        "    B, C, H, W = image.shape  # 画像のTensorの大きさを取得\n",
        "\n",
        "    oh_label = onehot_encode(label, device)  # ラベルをOne-hotベクトル化\n",
        "    oh_label = oh_label.expand(B, n_class, H, W)  # 画像サイズに合わせるようラベルを拡張する\n",
        "    return torch.cat((image, oh_label), dim=1)  # 画像とラベルをチャンネル方向(dim=1)で連結する\n",
        "```\n",
        "\n",
        "<br><br>生成器に入力するノイズにラベルを追加する関数を定義。\n",
        "\n",
        "<br>▽**ノイズとラベルの連結**\n",
        "\n",
        "\n",
        "```\n",
        "def concat_noise_label(noise, label, device):\n",
        "    \"\"\"\n",
        "    ノイズ（ランダムベクトル）とラベルを連結する\n",
        "    :param noize: ノイズ\n",
        "    :param label: ラベル\n",
        "    :param device: 楽手に使用するデバイス。CPUもしくはGPU\n",
        "    :return: ノイズとラベルを連結したTensor\n",
        "    \"\"\"\n",
        "    oh_label = onehot_encoding(label, device)  # ラベルをOne-hotベクトル化\n",
        "    return torch.cat((niose, oh_label), dim=1)  # ノイズとラベルをチャンネル方向(dim=1)で連結する\n",
        "\n",
        "```\n",
        "\n",
        "<br><br>確認用の画像50枚を生成するため、バッチサイズの数50の固定ノイズをあらかじめ作成する。固定したノイズをも一いて1エポックごとに生成画像を保存する。\n",
        "\n",
        "<br>▽**確認用生成画像のノイズ作成**\n",
        "\n",
        "```\n",
        "# 生成器のエポックごとの画像生成に使用する確認用の固定ノイズ\n",
        "fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "# 確認用のラベル。0～9のラベルの繰り返し\n",
        "fixed_label = [i for i in range(10)] * (batch_size // 10)\n",
        "fixed_label = torch.tensor(fixed_label, dtype=torch.long, device=device)\n",
        "# 確認用のノイズとラベルを連結\n",
        "fixed_noise_label = concat_noise_label(fixed_noise, fixed_label, device)\n",
        "print(fixed_noise.shape)\n",
        "print(fixed_label)\n",
        "print(fixed_noise_label.shape)\n",
        "```\n",
        "\n",
        "<br><br>本物画像real_imageとラベルreal_labelを関数concat_image_labelで連結し、real_image_labelを作成する。このときのTensorの形状は（B, 11, 28, 28）になる。\n",
        "<br>標準正規分布のランダムノイズnoiseにランダムなラベルfake_labelを関数concat_noise_labelで連結し、fake_noise_labelを作成する。このときのTensorの形状はTensor(B, 110, 1, 1)になる。\n",
        "\n",
        "<br>▽**学習の実行（画像とラベルの結合）**\n",
        "\n",
        "\n",
        "```\n",
        "# 学習のループ\n",
        "for epoch in range(n_epoch):\n",
        "    for itr, data in enumerate(dataloader):\n",
        "        real_image = data[0].to(device)  # 本物画像\n",
        "        real_label = data[1].to(device)  # 本物画像に対応するラベル\n",
        "        # 本物画像とラベルを連結\n",
        "        real_image_label = concat_image_label(real_image, real_label, device)\n",
        "        sample_size = real_image.size(0)  画像枚数\n",
        "\n",
        "        # 標準正規分布からノイズを生成\n",
        "        noise = torch.randn(sample_size, nz, 1, 1, device=device)\n",
        "        # 生成画像生成用のラベル\n",
        "        fake_label = torch.randint(10, (sample_size),\n",
        "        dtype=torch.long, device=device)\n",
        "        # ノイズとラベルを連結\n",
        "        fake_noise_label = concat_noise_label(noise, fake_label, device)\n",
        "        # 本物画像に対する識別信号の目標値[1]\n",
        "        real_target = torch.full((sample_size,), 1., device=device)\n",
        "        # 生成画像に対する識別信号の目標値[0]\n",
        "        fake_target = torch.full((sample_size,), 0., device=device)\n",
        "```\n",
        "\n",
        "<br><br>識別器netDに、画像とラベルを連結したreal_image_labelとfake_image_labelを入力する。生成器netGにfake_noise_labelを入力する。\n",
        "\n",
        "<br>▽学習の実行（識別器Dのパラメータ更新）\n",
        "\n",
        "\n",
        "```\n",
        "        ####################################\n",
        "        # 識別器Dの更新\n",
        "        ####################################\n",
        "        netD.zero_grad()  # 勾配の初期化\n",
        "\n",
        "        # 識別器Dで本物画像とラベルの組み合わせに対する識別信号を出力\n",
        "        output = netD(real_image_label)\n",
        "        # 本物画像に対する識別信号の損失値\n",
        "        errD_real = criterion(output, real_target)\n",
        "        D_x = output.mean().item() # 本物画像の識別番号の平均\n",
        "\n",
        "        # 生成器Gでラベルに対応した生成画像を生成\n",
        "        fake_image = netG(fake_noise_label)\n",
        "        # 生成画像とラベルを連結\n",
        "        fake_image_label = concat_image_label(fake_imag, fake_label, device)\n",
        "\n",
        "        # 識別器Dで生成画像に対する識別信号を出力\n",
        "        output = netD(fake_image_label.detach())\n",
        "        # 生成画像に対する識別信号の損失値\n",
        "        errD_fake = ciriterion(output, fake_target)\n",
        "        D_G_z1 = output.mean().item()  # 生成画像の識別信号の平均\n",
        "\n",
        "        errD = errD_real + errD_fake  # 識別器Dの全体の損失\n",
        "        errD.backward()  # 誤差逆伝播\n",
        "        optimizerD.step()  # Dのパラメータを更新\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5qJddaoa9Xg",
        "outputId": "5e4b09ca-90c6-449f-fe0d-f132b1c13020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator(\n",
            "  (layers): ModuleDict(\n",
            "    (layer0): Sequential(\n",
            "      (0): ConvTranspose2d(110, 512, kernel_size=(3, 3), stride=(1, 1))\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (layer1): Sequential(\n",
            "      (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2))\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (layers3): Sequential(\n",
            "      (0): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (1): Tanh()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   ConvTranspose2d-1            [-1, 512, 3, 3]         507,392\n",
            "       BatchNorm2d-2            [-1, 512, 3, 3]           1,024\n",
            "              ReLU-3            [-1, 512, 3, 3]               0\n",
            "   ConvTranspose2d-4            [-1, 256, 7, 7]       1,179,904\n",
            "       BatchNorm2d-5            [-1, 256, 7, 7]             512\n",
            "              ReLU-6            [-1, 256, 7, 7]               0\n",
            "   ConvTranspose2d-7          [-1, 128, 14, 14]         524,416\n",
            "       BatchNorm2d-8          [-1, 128, 14, 14]             256\n",
            "              ReLU-9          [-1, 128, 14, 14]               0\n",
            "  ConvTranspose2d-10            [-1, 1, 28, 28]           2,049\n",
            "             Tanh-11            [-1, 1, 28, 28]               0\n",
            "================================================================\n",
            "Total params: 2,215,553\n",
            "Trainable params: 2,215,553\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.98\n",
            "Params size (MB): 8.45\n",
            "Estimated Total Size (MB): 9.43\n",
            "----------------------------------------------------------------\n",
            "Discriminator(\n",
            "  (layers): ModuleDict(\n",
            "    (layer0): Sequential(\n",
            "      (0): Conv2d(11, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (layer1): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2))\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): Conv2d(512, 1, kernel_size=(3, 3), stride=(1, 1))\n",
            "      (1): Sigmoid()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 128, 14, 14]          22,656\n",
            "         LeakyReLU-2          [-1, 128, 14, 14]               0\n",
            "            Conv2d-3            [-1, 256, 7, 7]         524,544\n",
            "       BatchNorm2d-4            [-1, 256, 7, 7]             512\n",
            "         LeakyReLU-5            [-1, 256, 7, 7]               0\n",
            "            Conv2d-6            [-1, 512, 3, 3]       1,180,160\n",
            "       BatchNorm2d-7            [-1, 512, 3, 3]           1,024\n",
            "         LeakyReLU-8            [-1, 512, 3, 3]               0\n",
            "            Conv2d-9              [-1, 1, 1, 1]           4,609\n",
            "          Sigmoid-10              [-1, 1, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 1,733,505\n",
            "Trainable params: 1,733,505\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.03\n",
            "Forward/backward pass size (MB): 0.78\n",
            "Params size (MB): 6.61\n",
            "Estimated Total Size (MB): 7.42\n",
            "----------------------------------------------------------------\n",
            "torch.Size([50, 100, 1, 1])\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "torch.Size([50, 110, 1, 1])\n",
            "[1/10][1/1200] Loss_D:1.808 Loss_G:1.692 D(x):0.301 D(G(z)):0.376/0.644\n",
            "[1/10][601/1200] Loss_D:1.242 Loss_G:1.006 D(x):0.634 D(G(z)):0.535/0.369\n",
            "[2/10][1/1200] Loss_D:1.323 Loss_G:0.612 D(x):0.626 D(G(z)):0.573/0.543\n",
            "[2/10][601/1200] Loss_D:1.369 Loss_G:0.830 D(x):0.473 D(G(z)):0.460/0.438\n",
            "[3/10][1/1200] Loss_D:1.624 Loss_G:0.734 D(x):0.365 D(G(z)):0.454/0.480\n",
            "[3/10][601/1200] Loss_D:1.575 Loss_G:0.578 D(x):0.502 D(G(z)):0.586/0.562\n",
            "[4/10][1/1200] Loss_D:1.521 Loss_G:0.718 D(x):0.413 D(G(z)):0.470/0.489\n",
            "[4/10][601/1200] Loss_D:1.433 Loss_G:0.727 D(x):0.475 D(G(z)):0.492/0.483\n",
            "[5/10][1/1200] Loss_D:1.432 Loss_G:0.744 D(x):0.516 D(G(z)):0.530/0.479\n",
            "[5/10][601/1200] Loss_D:1.270 Loss_G:0.731 D(x):0.534 D(G(z)):0.472/0.481\n",
            "[6/10][1/1200] Loss_D:1.184 Loss_G:0.841 D(x):0.550 D(G(z)):0.443/0.432\n",
            "[6/10][601/1200] Loss_D:1.167 Loss_G:0.808 D(x):0.574 D(G(z)):0.457/0.446\n",
            "[7/10][1/1200] Loss_D:1.315 Loss_G:0.834 D(x):0.476 D(G(z)):0.431/0.435\n",
            "[7/10][601/1200] Loss_D:1.402 Loss_G:0.945 D(x):0.384 D(G(z)):0.355/0.390\n",
            "[8/10][1/1200] Loss_D:1.297 Loss_G:0.749 D(x):0.515 D(G(z)):0.468/0.473\n",
            "[8/10][601/1200] Loss_D:1.276 Loss_G:1.013 D(x):0.427 D(G(z)):0.340/0.368\n",
            "[9/10][1/1200] Loss_D:1.252 Loss_G:0.734 D(x):0.564 D(G(z)):0.493/0.480\n",
            "[9/10][601/1200] Loss_D:1.332 Loss_G:0.698 D(x):0.533 D(G(z)):0.503/0.498\n",
            "[10/10][1/1200] Loss_D:1.250 Loss_G:0.772 D(x):0.532 D(G(z)):0.462/0.462\n",
            "[10/10][601/1200] Loss_D:1.006 Loss_G:0.817 D(x):0.662 D(G(z)):0.448/0.442\n"
          ]
        }
      ],
      "source": [
        "# パッケージのインポート\n",
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import torchsummary\n",
        "\n",
        "# 設定\n",
        "workers = 2\n",
        "batch_size = 50\n",
        "nz = 100\n",
        "nch_g = 128\n",
        "nch_d = 128\n",
        "n_epoch = 10\n",
        "lr = 0.002\n",
        "beta1 = 0.5\n",
        "outf = './result_3_3-CGAN'\n",
        "display_interval = 600\n",
        "\n",
        "# 保存先ディレクトリを作成\n",
        "try:\n",
        "    os.makedirs(outf, exist_ok=True)\n",
        "except OSError as error:\n",
        "    print(error)\n",
        "    pass\n",
        "\n",
        "# MNISTの訓練データセットを読み込む\n",
        "dataset = dset.MNIST(root='./mnist_root', download=True, train=True,\n",
        "                     transform=transforms.Compose([\n",
        "                         transforms.ToTensor(),\n",
        "                         transforms.Normalize((0.5,), (0.5,))]))\n",
        "\n",
        "# 画像配列の確認\n",
        "dataset[0][0].shape\n",
        "\n",
        "# 訓練データをセットしたデータローダーを作成する\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=int(workers))\n",
        "\n",
        "# 学習に使用するデバイスを得る。可能ならGPUを使用する\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ネットワークの定義\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    生成器のクラス\n",
        "    \"\"\"\n",
        "    def __init__(self, nz=100, nch_g=64, nch=1):\n",
        "        \"\"\"\n",
        "        :param nz: 入力ベクトルzの次元\n",
        "        :param nch_g: 最終層の入力チャネル数\n",
        "        :param nch: 出力画像のチャネル数\n",
        "        \"\"\"\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # ニューラルネットワークの構造を定義する\n",
        "        self.layers = nn.ModuleDict({\n",
        "              \"layer0\":nn.Sequential(\n",
        "                  nn.ConvTranspose2d(nz, nch_g * 4, 3, 1, 0), # 転置畳み込み\n",
        "                  nn.BatchNorm2d(nch_g * 4),                  # バッチノーマライゼーション\n",
        "                  nn.ReLU()                                   # ReLU\n",
        "              ),  # (B, nz, 1, 1) -> (B, nch_g*4, 3, 3)\n",
        "              \"layer1\":nn.Sequential(\n",
        "                  nn.ConvTranspose2d(nch_g * 4, nch_g * 2, 3, 2, 0),\n",
        "                  nn.BatchNorm2d(nch_g * 2),\n",
        "                  nn.ReLU()\n",
        "              ),  # (B, nch_g*4, 3, 3) -> (B, nch_g*2, 7, 7)\n",
        "              \"layer2\":nn.Sequential(\n",
        "                  nn.ConvTranspose2d(nch_g * 2, nch_g, 4, 2, 1),\n",
        "                  nn.BatchNorm2d(nch_g),\n",
        "                  nn.ReLU()\n",
        "              ),  # (B, nch_g*2, 7, 7) -> (B, nch_g, 14, 14)\n",
        "              \"layers3\":nn.Sequential(\n",
        "                  nn.ConvTranspose2d(nch_g, nch, 4, 2, 1),\n",
        "                  nn.Tanh()\n",
        "              )   # (B, nch_g, 14, 14) -> (B, nch, 28, 28)\n",
        "        })\n",
        "\n",
        "    def forward(self, z):\n",
        "         \"\"\"\n",
        "         順方向の演算\n",
        "         :param z: 入力ベクトル\n",
        "         :return: 生成画像\n",
        "         \"\"\"\n",
        "         for layer in self.layers.values():   # self.layersの各層で演算を行う\n",
        "             z = layer(z)\n",
        "         return z\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "      \"\"\"\n",
        "      識別器Dのクラス\n",
        "      \"\"\"\n",
        "      def __init__(self, nch=1, nch_d=64):\n",
        "          \"\"\"\n",
        "          :param nch: 入力画像のチャネル数\n",
        "          :param nch_d: 先頭層の出力チャネル数\n",
        "          \"\"\"\n",
        "          super(Discriminator, self).__init__()\n",
        "\n",
        "          # ニューラルネットワークの構造を定義する\n",
        "          self.layers = nn.ModuleDict({\n",
        "            \"layer0\":nn.Sequential(\n",
        "                nn.Conv2d(nch, nch_d, 4, 2, 1),   # 畳み込み\n",
        "                nn.LeakyReLU(negative_slope=0.2)  # leaky ReLU関数\n",
        "            ),  # (B, nch, 28, 28) -> (B, nch_d, 14, 14)\n",
        "            \"layer1\":nn.Sequential(\n",
        "                nn.Conv2d(nch_d, nch_d * 2, 4, 2, 1),\n",
        "                nn.BatchNorm2d(nch_d * 2),\n",
        "                nn.LeakyReLU(negative_slope=0.2)\n",
        "            ),  # (B, nch_d, 14, 14) -> (B, nch_d*2, 7, 7)\n",
        "            \"layer2\":nn.Sequential(\n",
        "                nn.Conv2d(nch_d * 2, nch_d * 4, 3, 2, 0),\n",
        "                nn.BatchNorm2d(nch_d * 4),\n",
        "                nn.LeakyReLU(negative_slope=0.2)\n",
        "            ),  # (B, nch_d*2, 7, 7) -> (B, nch_d*4, 3, 3)\n",
        "            \"layer3\":nn.Sequential(\n",
        "                nn.Conv2d(nch_d * 4, 1, 3, 1, 0),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "            # (B, nch_d*4, 3, 3) -> (B, 1, 1, 1)\n",
        "          })\n",
        "\n",
        "      def forward(self, x):\n",
        "          \"\"\"\n",
        "          順方向の演算\n",
        "          :param x: 本物画像あるいは生成画像\n",
        "          :return: 識別信号\n",
        "          \"\"\"\n",
        "          for layer in self.layers.values(): # self.layersの各層で演算を行う\n",
        "              x = layer(x)\n",
        "          return x.squeeze()  # Tensorの形状を(B)に変更して戻り値とする\n",
        "\n",
        "def weights_init(m):\n",
        "      \"\"\"\n",
        "      ニューラルネットワークの重みを初期化する。作成したインスタンスに対しapplyメソッドで適用する\n",
        "      :param m: ニューラルネットワークを構成する層\n",
        "      \"\"\"\n",
        "      classname = m.__class__.__name__\n",
        "      if classname.find(\"Conv\") != -1:        # 畳み込み層の場合\n",
        "            m.weight.data.normal_(0.0, 0.02)\n",
        "            m.bias.data.fill_(0)\n",
        "      elif classname.find(\"Liner\") != -1:     # 全結合層の場合\n",
        "            m.weight.data.normal(0.0, 0.02)\n",
        "            m.bias.data.fill_(0)\n",
        "      elif classname.find(\"BatchNorm\") != -1: # バッチノーマライゼーションの場合\n",
        "            m.weight.data.normal_(1.0, 0.02)\n",
        "            m.bias.data.fill_(0)\n",
        "\n",
        "# 生成器G。ランダムベクトルから生成画像を作成する\n",
        "netG = Generator(nz=nz+10, nch_g=nch_g,).to(device) # 10はn_class=10を指す。出し分けに必要なラベル情報\n",
        "netG.apply(weights_init)  # weights_init関数で初期化\n",
        "print(netG)\n",
        "\n",
        "# 生成器GのTensor形状\n",
        "torchsummary.summary(netG, (110, 1, 1))\n",
        "\n",
        "# 識別器D。画像が本物か生成かを識別する\n",
        "netD = Discriminator(nch=1+10, nch_d=nch_d).to(device)  # 10はn_class=10を指す。分類に必要なラベル情報\n",
        "netD.apply(weights_init)\n",
        "print(netD)\n",
        "\n",
        "# 識別器DのTensor形状\n",
        "torchsummary.summary(netD, (11, 28, 28))\n",
        "\n",
        "# 学習の実行\n",
        "criterion = nn.BCELoss()  # バイナリークロスエントロピー（Sigmoid関数無し）\n",
        "\n",
        "# オプティマイザのセットアップ\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999), weight_decay=1e-5)  # 識別器D用\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999), weight_decay=1e-5)  # 生成器G用\n",
        "\n",
        "def onehot_encode(label, device, n_class=10):\n",
        "      \"\"\"\n",
        "      カテゴリカル変数のラベルをOne-hot形式に変換する\n",
        "      :param label: 変換対象のラベル\n",
        "      :param device: 学習に使用するデバイス。CPUあるいはGPU\n",
        "      :param n_class: ラベルのクラス数\n",
        "      :return:\n",
        "      \"\"\"\n",
        "      eye = torch.eye(n_class, device=device)\n",
        "      # ランダムベクトルあるいは画像と連結するために(B, c_class, 1, 1)のTensorにして戻す\n",
        "      return eye[label].view(-1, n_class, 1, 1)\n",
        "\n",
        "def concat_image_label(image, label, device, n_class=10):\n",
        "      \"\"\"\n",
        "      画像とラベルを連結する\n",
        "      :param image: 画像\n",
        "      :param label: ラベル\n",
        "      :param device: 学習に使用するデバイス。CPUあるいはGPU\n",
        "      :param n_class: ラベルのクラス数\n",
        "      :return: 画像とラベルをチャネル方向に連結したTensor\n",
        "      \"\"\"\n",
        "      B, C, H, W = image.shape  # 画像Tensorの大きさを取得\n",
        "\n",
        "      oh_label = onehot_encode(label, device)  # ラベルをOne-Hotベクトル化\n",
        "      oh_label = oh_label.expand(B, n_class, H, W)  # 画像のサイズに合わせるようラベルを拡張する\n",
        "      return torch.cat((image, oh_label), dim=1)  # 画像とラベルをチャネル方向（dim=1）で連結する\n",
        "\n",
        "def concat_noise_label(noise, label, device):\n",
        "      \"\"\"\n",
        "      ノイズ（ランダムベクトル）とラベルを連結する\n",
        "      :param noise: ノイズ\n",
        "      :param label: ラベル\n",
        "      :param device: 学習に使用するデバイス。CPUまたはGPU\n",
        "      :return: ノイズとラベルを連結したTensor\n",
        "      \"\"\"\n",
        "      oh_label = onehot_encode(label, device)  # ラベルをOne-Hotベクトル化\n",
        "      return torch.cat((noise, oh_label), dim=1)  # ノイズとラベルをチャネル方向（dim=1）で連結する\n",
        "\n",
        "# 生成器のエポックごとの画像生成に使用する確認用の固定のノイズ\n",
        "fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "# 確認用のラベル。0～9のラベルの繰り返し\n",
        "fixed_label = [i for i in range(10)] * (batch_size // 10)\n",
        "fixed_noise_label = torch.tensor(fixed_label, dtype=torch.long, device=device)\n",
        "# 確認用のノイズとラベル\n",
        "fixed_noise_label = concat_noise_label(fixed_noise, fixed_label, device)\n",
        "print(fixed_noise.shape)\n",
        "print(fixed_label)\n",
        "print(fixed_noise_label.shape)\n",
        "\n",
        "# 学習のループ\n",
        "for epoch in range(n_epoch):\n",
        "      for itr, data in enumerate(dataloader):\n",
        "            real_image = data[0].to(device)  # 本物画像\n",
        "            real_label = data[1].to(device)  # 本物画像に対応するラベル\n",
        "            # 本物画像とラベルを連結\n",
        "            real_image_label = concat_image_label(real_image, real_label, device)\n",
        "            sample_size = real_image.size(0)  # 画像枚数\n",
        "\n",
        "            # 標準正規分布からノイズを作成\n",
        "            noise = torch.randn(sample_size, nz, 1, 1, device=device)\n",
        "            #生成画像生成用のラベル\n",
        "            fake_label = torch.randint(10, (sample_size,), dtype=torch.long, device=device)\n",
        "            # ノイズとラベルを連結\n",
        "            fake_noise_label = concat_noise_label(noise, fake_label, device)\n",
        "            # 本物画像に対する識別信号の目標値[1]\n",
        "            real_target = torch.full((sample_size,), 1., device=device)\n",
        "            # 生成画像に対する識別信号の目標値[0]\n",
        "            fake_target = torch.full((sample_size,), 0., device=device)\n",
        "\n",
        "            #########################\n",
        "            # 識別器Dの更新\n",
        "            #########################\n",
        "            netD.zero_grad()  # 勾配の初期化\n",
        "\n",
        "            # 識別器Dで本物画像とラベルの組み合わせに対する識別信号を出力\n",
        "            output = netD(real_image_label)\n",
        "            # 本物画像に対する識別信号の損失値\n",
        "            errD_real = criterion(output, real_target)\n",
        "\n",
        "            D_x = output.mean().item()  # 本物画像の識別信号の平均\n",
        "\n",
        "            fake_image = netG(fake_noise_label)  # 生成器Gでラベルに対応した生成画像を生成\n",
        "            # 生成画像とラベルを連結\n",
        "            fake_image_label = concat_image_label(fake_image, fake_label, device)\n",
        "\n",
        "            # 識別器Dで本物画像に対する識別信号を出力\n",
        "            output = netD(fake_image_label.detach())\n",
        "            # 生成画像に対する識別番号の損失値\n",
        "            errD_fake = criterion(output, fake_target)\n",
        "            D_G_z1 = output.mean().item() # 生成画像の識別信号の平均\n",
        "\n",
        "            errD = errD_real + errD_fake  # 識別器Dの全体の損失\n",
        "            errD.backward()  # 誤差逆伝播\n",
        "            optimizerD.step()  # Dのパラメータを更新\n",
        "\n",
        "            #########################\n",
        "            # 識別器Dの更新\n",
        "            #########################\n",
        "            netG.zero_grad()  # 勾配の初期化\n",
        "\n",
        "            output = netD(fake_image_label)   # 更新した識別器Dで改めて生成画像とラベルの組み合わせに対する識別信号を出力\n",
        "            errG = criterion(output, real_target)   # 生成器Gの損失値。Dに生成画像を本物画像と誤認させたいため目標値は「1」\n",
        "            errG.backward()  # 誤差逆伝播\n",
        "            D_G_z2 = output.mean().item()  #更新した識別器Dによる生成画像を本物画像と認識させたいため 目標値は[0]\n",
        "\n",
        "            optimizerG.step()  # Gのパラメータを更新\n",
        "\n",
        "            if itr % display_interval == 0:\n",
        "                  print(\"[{}/{}][{}/{}] Loss_D:{:.3f} Loss_G:{:.3f} D(x):{:.3f} D(G(z)):{:.3f}/{:.3f}\"\n",
        "                  .format(epoch + 1, n_epoch,\n",
        "                          itr +1, len(dataloader),\n",
        "                          errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "            if epoch == 0 and itr == 0:\n",
        "                  vutils.save_image(real_image, \"{}/real_samples.png\".format(outf),\n",
        "                                    normalize=True, nrow=10)\n",
        "\n",
        "######################\n",
        "# 確認用画像の生成\n",
        "######################\n",
        "fake_image = netG(fixed_noise_label)   # 1エポック終了ごとに確認用の生成画像を生成する\n",
        "vutils.save_image(fake_image.detach(), \"{}/fake_samples_epoch_{:03d}.png\".format(outf, epoch + 1),\n",
        "                  normalize=True, nrow=10)\n",
        "\n",
        "######################\n",
        "# モデルの保存\n",
        "######################\n",
        "if (epoch + 1) % 10 == 0:\n",
        "    torch.save(netG.state_dict(), \"{}/netG_epoch_{}.pth\".format(outf, epoch + 1))\n",
        "    torch.save(netD.state_dict(), \"{}/netD_epoch_{}.pth\".format(outf, epoch + 1))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2o9Y2/PCnwY7/sWaDPAzo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}